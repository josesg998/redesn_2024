{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "id": "IbhfijmvwYv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ],
      "metadata": {
        "id": "A7qUmpiS1tfe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print(keras.__version__)  # Necesitamos tener la versión 3 de keras"
      ],
      "metadata": {
        "id": "bDmnG0gWwtHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Cb0M4oc7YIV0"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Activation, Dense, Input\n",
        "from keras.layers import Conv2D, Flatten, Layer\n",
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "from keras.layers import MaxPooling2D,UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import ops\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage.transform import resize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones útiles\n",
        "\n",
        "def load_data():\n",
        "  (x_train, _), (x_test, _) = mnist.load_data()\n",
        "  input_dim = 28*28\n",
        "  x_train = np.reshape(x_train, [-1, input_dim])/255.0\n",
        "  x_test = np.reshape(x_test,   [-1, input_dim])/255.0\n",
        "\n",
        "  return x_train,x_test,input_dim\n",
        "\n",
        "def show_as_single_image(imgs,image_shape=None):\n",
        "  if image_shape is None:\n",
        "    n,h,w,c=imgs.shape\n",
        "  else:\n",
        "    h,w=image_shape\n",
        "    n,c=imgs.shape\n",
        "  imgs = imgs.reshape((n, h,w))\n",
        "  imgs = imgs.swapaxes(1,2).reshape((n*w, h)).swapaxes(0,1)\n",
        "  imgs = (imgs * 255).astype(np.uint8)\n",
        "\n",
        "  plt.figure(figsize=(1*8,n*8),dpi=100)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(imgs, interpolation='none', cmap='gray')\n",
        "  plt.tight_layout()\n",
        "\n",
        "def resize_images(x,size):\n",
        "  n=x.shape[0]\n",
        "  h,w=size\n",
        "  new_x=np.zeros((n,h,w,1))\n",
        "  for i in range(n):\n",
        "    new_x[i,:,:,0]=resize(x[i,:,:],size).astype(np.float32)\n",
        "  return new_x\n",
        "\n",
        "def load_data_conv():\n",
        "  size=32,32\n",
        "  (x_train, _), (x_test, _) = mnist.load_data()\n",
        "  print(f\"Resizing training images to {size}...\")\n",
        "  x_train=resize_images(x_train,size)\n",
        "  print(f\"Resizing test images to {size}...\")\n",
        "  x_test=resize_images(x_test,size)\n",
        "  input_shape = x_train.shape[1:]\n",
        "  return x_train,x_test,input_shape\n",
        "\n",
        "def show_images_conv(x,columns):\n",
        "  if x.shape[3]==1:\n",
        "    x = x.reshape(*x.shape[:3])\n",
        "  n = x.shape[0]\n",
        "  rows = n //  columns\n",
        "  if (n % columns) > 0:\n",
        "    rows+=1\n",
        "  rows = max(rows,1)\n",
        "  f, axes = plt.subplots(rows,columns,figsize=(columns,rows),dpi=100,squeeze=False)\n",
        "  for i in range(rows):\n",
        "    for j in range(columns):\n",
        "      index = i*columns+j\n",
        "      if index<n:\n",
        "          axes[i,j].imshow(x[index,:],cmap=\"gray\")\n",
        "      axes[i,j].set_axis_off()"
      ],
      "metadata": {
        "id": "5poLSDK3YgN5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder basado en capas Dense"
      ],
      "metadata": {
        "id": "49D-sF6KYTsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de datos\n",
        "\n",
        "x_train,x_test,input_dim = load_data()"
      ],
      "metadata": {
        "id": "94fo3TP4jk2K"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de features del espacio latente\n",
        "\n",
        "latent_dim = 16"
      ],
      "metadata": {
        "id": "Cpi5D_EWjkV7"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DenseAutoencoder(input_dim,latent_dim):\n",
        "  def generate_encoder():\n",
        "    encoder_input = Input(shape=(input_dim,), name='encoder_input')\n",
        "    code = Dense(latent_dim, name='latent_vector')(encoder_input)\n",
        "    encoder = Model(encoder_input, code, name='encoder')\n",
        "    return encoder,encoder_input\n",
        "\n",
        "  def generate_decoder():\n",
        "    latent_input = Input(shape=(latent_dim,), name='decoder_input')\n",
        "    decoded_image = Dense(input_dim,activation=\"sigmoid\",name='decoder_output')(latent_input)\n",
        "    decoder = Model(latent_input, decoded_image, name='decoder')\n",
        "    return decoder\n",
        "\n",
        "  encoder,encoder_input = generate_encoder()\n",
        "  decoder = generate_decoder()\n",
        "  autoencoder = Model(encoder_input, decoder(encoder(encoder_input)), name='autoencoder')\n",
        "  return autoencoder,encoder,decoder\n",
        "\n",
        "autoencoder,encoder,decoder=DenseAutoencoder(input_dim,latent_dim)\n",
        "\n",
        "autoencoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "print(autoencoder.summary())\n",
        "print(encoder.summary())\n",
        "print(decoder.summary())"
      ],
      "metadata": {
        "id": "pjB7WkbRYry6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar el entrenamiento\n",
        "\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "autoencoder.fit(x_train, x_train, validation_data=(x_test, x_test), epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "W6j5uUzEY9Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar la predicción\n",
        "\n",
        "x_decoded = autoencoder.predict(x_test)"
      ],
      "metadata": {
        "id": "yZlp8WJLZMuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las imágenes originales y sus respectivas reconstrucciones\n",
        "\n",
        "num=10 # cantidad de imágenes a visualizar\n",
        "print(\"Originales\")\n",
        "show_as_single_image(x_test[:num],(28,28))\n",
        "print(\"Restauradas\")\n",
        "show_as_single_image(x_decoded[:num],(28,28))"
      ],
      "metadata": {
        "id": "OMtJjnNeZ0wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar nuevas muestras completamente aleatorias desde el espacio latente\n",
        "\n",
        "num_images=10 # Número de imágenes a generar\n",
        "\n",
        "random_latent_code = np.random.random_sample([num_images,latent_dim])\n",
        "\n",
        "random_images = decoder.predict(random_latent_code)\n",
        "\n",
        "print(\"Imágenes generadas aleatoriamente\")\n",
        "show_as_single_image(random_images,(28,28))"
      ],
      "metadata": {
        "id": "q-AQR0aoaJWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder basado en capas Convolucionales"
      ],
      "metadata": {
        "id": "CqFiiYs1bew_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de datos\n",
        "\n",
        "x_train,x_test,input_shape = load_data_conv()"
      ],
      "metadata": {
        "id": "BkKKt3fAcYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de features del espacio latente\n",
        "\n",
        "latent_dim = 16"
      ],
      "metadata": {
        "id": "vIT6OECMlTFS"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ConvolutionalAutoencoder(input_shape,latent_dim):\n",
        "  filters = 32\n",
        "\n",
        "  def generate_encoder():\n",
        "    encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "    x = encoder_input\n",
        "    x = Conv2D(filters*4, (3, 3), activation='relu', padding='same')(encoder_input)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(filters*2, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    encoded_shape = ops.shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(latent_dim)(x)\n",
        "\n",
        "    encoder = Model(encoder_input, x, name='encoder')\n",
        "    return encoder,encoder_input,encoded_shape\n",
        "\n",
        "  def generate_decoder(encoded_shape):\n",
        "    latent_input = Input(shape=(latent_dim,), name='decoder_input')\n",
        "    x = Dense(np.prod(encoded_shape))(latent_input)\n",
        "    x = Reshape(encoded_shape)(x)\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(filters*2, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(filters*4, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    decoder = Model(latent_input, x, name='decoder')\n",
        "    return decoder,latent_input\n",
        "\n",
        "  encoder,encoder_input,encoded_shape = generate_encoder()\n",
        "  decoder,latent_input = generate_decoder(encoded_shape)\n",
        "  autoencoder = Model(encoder_input, decoder(encoder(encoder_input)), name='autoencoder')\n",
        "  return autoencoder,encoder,decoder,encoded_shape\n",
        "\n",
        "autoencoder,encoder,decoder,encoded_shape=ConvolutionalAutoencoder(input_shape,latent_dim=latent_dim)\n",
        "autoencoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "print(autoencoder.summary())\n",
        "print(encoder.summary())\n",
        "print(decoder.summary())"
      ],
      "metadata": {
        "id": "cQ9GCFSZbuBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar el entrenamiento\n",
        "\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "history = autoencoder.fit(x_train, x_train, validation_data=(x_test, x_test), epochs = epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "mQSoo_xBfMbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar la predicción\n",
        "\n",
        "x_decoded = autoencoder.predict(x_test)"
      ],
      "metadata": {
        "id": "ujMdYTaLfW2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las imágenes originales y sus respectivas reconstrucciones\n",
        "\n",
        "num=10 # cantidad de imágenes a visualizar\n",
        "print(\"Original\")\n",
        "show_images_conv(x_test[:num],columns=num)\n",
        "print(\"Decoded\")\n",
        "show_images_conv(x_decoded[:num],columns=num)"
      ],
      "metadata": {
        "id": "VZBUXmXpfdxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar nuevas muestras completamente aleatorias desde el espacio latente\n",
        "\n",
        "num_images=10 # Número de imágenes a generar\n",
        "\n",
        "random_latent_code = np.random.random_sample([num_images,latent_dim])\n",
        "\n",
        "random_images = decoder.predict(random_latent_code)\n",
        "\n",
        "show_images_conv(random_images,columns=num_images)"
      ],
      "metadata": {
        "id": "0JTrvzMaia-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoencoder variacional"
      ],
      "metadata": {
        "id": "2Td6Jq9Yt6Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de datos\n",
        "\n",
        "x_train,x_test,input_shape = load_data_conv()"
      ],
      "metadata": {
        "id": "5XB6Ar-At9bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de features del espacio latente\n",
        "\n",
        "latent_dim = 8"
      ],
      "metadata": {
        "id": "TtzjPO8AuKze"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construimos una capa que haga el sampleo\n",
        "\n",
        "class Sampling(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = ops.shape(z_mean)[0]\n",
        "        dim = ops.shape(z_mean)[1]\n",
        "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
        "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "Gi81Sv7Jvat-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoencoder(Model):\n",
        "  def generate_encoder(this):\n",
        "    encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "    x = Conv2D(this.filters*4, (3, 3), activation='relu', strides=2, padding='same')(encoder_input)\n",
        "    x = Conv2D(this.filters*8, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
        "    encoded_shape = ops.shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(16, activation=\"relu\")(x)\n",
        "\n",
        "    z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "    z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    encoder = Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    return encoder,encoded_shape\n",
        "\n",
        "  def generate_decoder(this):\n",
        "    latent_input = Input(shape=(latent_dim,), name='decoder_input')\n",
        "    x = Dense(np.prod(this.encoded_shape), activation=\"relu\")(latent_input)            #x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "    x = Reshape(this.encoded_shape)(x)                                                 #x = layers.Reshape((7, 7, 64))(x)\n",
        "    x = Conv2DTranspose(this.filters*8, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    x = Conv2DTranspose(this.filters*4, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    decoder_outputs = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "    decoder = Model(latent_input, decoder_outputs, name=\"decoder\")\n",
        "    return decoder\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.filters = 8\n",
        "        self.encoder, self.encoded_shape = self.generate_encoder()\n",
        "        self.decoder = self.generate_decoder()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "  def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = ops.mean(\n",
        "                ops.sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2),\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
        "            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "autoencoder = VariationalAutoencoder()\n",
        "autoencoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "pcjfJ-PluRuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar el entrenamiento\n",
        "\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "history = autoencoder.fit(x_train, epochs = epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "jSFXJfrMAMUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar nuevas muestras completamente aleatorias desde el espacio latente\n",
        "\n",
        "num_images=10 # Número de imágenes a generar\n",
        "\n",
        "random_latent_code = np.random.random_sample([num_images,latent_dim])\n",
        "\n",
        "random_images = autoencoder.decoder.predict(random_latent_code)\n",
        "\n",
        "show_images_conv(random_images,columns=num_images)"
      ],
      "metadata": {
        "id": "322LnSBMG0wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar nuevas muestras completamente aleatorias desde el espacio latente (pero muestreando con las distribuciones conseguidas)\n",
        "\n",
        "num_images=10 # Número de imágenes a generar\n",
        "\n",
        "randoms = keras.random.normal(shape=(num_images, latent_dim))\n",
        "z_mean, z_log_var, z = autoencoder.encoder(x_train[0:1,::])\n",
        "random_latent_code = z_mean + ops.exp(0.5 * z_log_var) * randoms\n",
        "\n",
        "random_images = autoencoder.decoder.predict(random_latent_code)\n",
        "\n",
        "show_images_conv(random_images,columns=num_images)"
      ],
      "metadata": {
        "id": "oG5zZLKDKQ17"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}