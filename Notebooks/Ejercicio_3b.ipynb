{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDOS0BnHVyZE"
      },
      "source": [
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "from keras.utils import pad_sequences\n",
        "import nltk\n",
        "import keras\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKz-v7mwV2yK"
      },
      "source": [
        "# Lectura del dataset de train\n",
        "trainfile = csv.reader(open(\"train.csv\"), delimiter='\\t')\n",
        "trainrows = np.array([[c for c in row] for row in trainfile])\n",
        "row_count_train, column_count = np.shape(trainrows)\n",
        "T_train = [int(c) for c in trainrows[:, 0]]\n",
        "P_train = trainrows[:, 1]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdBggoUEZgP-"
      },
      "source": [
        "# Preprocesamiento de los textos\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# Eliminación de stop-wrods y stemming de los términos\n",
        "P_train = [re.sub(\"[^a-zA-Z]\", \" \", l.lower()) for l in P_train]\n",
        "P_train = [l.split() for l in P_train]\n",
        "P_train = [[lemmatizer.lemmatize(l) for l in row if l not in stopwords] for row in P_train]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensayo 1<BR>\n",
        "Creación del modelo de word2vec."
      ],
      "metadata": {
        "id": "Et4bqlr017g6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO3uMg4ke1Pj"
      },
      "source": [
        "embedding_size = 256\n",
        "model = Word2Vec(sentences = P_train, vector_size=embedding_size, min_count=3, window=5)\n",
        "\n",
        "# Convertir features word2vec\n",
        "\n",
        "vocab = model.wv.key_to_index\n",
        "keys = list(vocab.keys())\n",
        "filter_unknown = lambda word: vocab.get(word, None) is not None\n",
        "encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
        "word_vector = list(map(encode, P_train))\n",
        "\n",
        "input_length = 150\n",
        "P = pad_sequences(sequences=word_vector, maxlen=input_length, padding='post')\n",
        "T = np.array(T_train)\n",
        "print(P.shape)\n",
        "print(T.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensayo 2<BR>\n",
        "Creación del modelo de bag of words"
      ],
      "metadata": {
        "id": "jfrFAAcS4csT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mCIUNu4vGp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06796e9-230e-4427-c89d-b89f11fbe45b"
      },
      "source": [
        "input_length = 1000\n",
        "cv = CountVectorizer(max_features= input_length)\n",
        "P_train_2 = [\"\".join(row) for row in P_train]\n",
        "P_feat_train = cv.fit_transform(P_train_2).toarray()\n",
        "\n",
        "cv._validate_vocabulary()\n",
        "\n",
        "P_feat_train = [\"\".join(row) for row in P_train]\n",
        "P = cv.transform(P_feat_train).toarray()\n",
        "T = np.array(T_train)\n",
        "print(P.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2758, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento del modelo\n",
        "Solo usar una preparación de los datos (Prep1 o Prep2)"
      ],
      "metadata": {
        "id": "uP6kvhdY6iQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mezclar las muestras\n",
        "inds = np.arange(len(T))\n",
        "inds = np.random.permutation(inds)\n",
        "P = P[inds]\n",
        "T = T[inds]\n",
        "print(P.shape)\n",
        "print(T.shape)"
      ],
      "metadata": {
        "id": "Zt4-Kdeq_G_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in = P.shape[1]\n",
        "d_out = 1 # Clasificación binaria\n",
        "\n",
        "modelo = keras.Sequential([\n",
        "    keras.layers.Dense(5, input_shape=(d_in,), activation= 'tanh'),\n",
        "    keras.layers.Dense(d_out, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "modelo.compile(\n",
        "  optimizer = keras.optimizers.SGD(learning_rate=0.01),\n",
        "  loss = 'binary_crossentropy', metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "modelo.summary()"
      ],
      "metadata": {
        "id": "z4cWIRGl9xhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = modelo.fit(P, T, epochs=3, batch_size=16, verbose=True, validation_split=0.2)"
      ],
      "metadata": {
        "id": "8KUvTeke-iSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}